{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "278d1edba9794abdbb34a3bd1783f758": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_90e02a8549c741b89e9ace3fcea423e6",
              "IPY_MODEL_3329b21f272d471d824cc75c0ca17681",
              "IPY_MODEL_7a509a698deb4bf498cdf454c53c9b9c"
            ],
            "layout": "IPY_MODEL_7b4c6f27589d4d2b909c8bfa6c34a314"
          }
        },
        "90e02a8549c741b89e9ace3fcea423e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6aafe80177c471395de7fc7218caccd",
            "placeholder": "​",
            "style": "IPY_MODEL_c23c670538bf4209ba8246fcc34b7ae3",
            "value": "Map: 100%"
          }
        },
        "3329b21f272d471d824cc75c0ca17681": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_179f5fbac77e4fd59a5ecf5e7fb51329",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cabbd64074c342818fa60158bc27b0d0",
            "value": 2
          }
        },
        "7a509a698deb4bf498cdf454c53c9b9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_144e115d3c2b4aa9b53eca9dd499159f",
            "placeholder": "​",
            "style": "IPY_MODEL_62c0f3c61b634044abfe68f90e7b16bb",
            "value": " 2/2 [00:00&lt;00:00, 52.86 examples/s]"
          }
        },
        "7b4c6f27589d4d2b909c8bfa6c34a314": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6aafe80177c471395de7fc7218caccd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c23c670538bf4209ba8246fcc34b7ae3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "179f5fbac77e4fd59a5ecf5e7fb51329": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cabbd64074c342818fa60158bc27b0d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "144e115d3c2b4aa9b53eca9dd499159f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62c0f3c61b634044abfe68f90e7b16bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_lotoya5moD",
        "outputId": "56b03a46-631b-4dae-b783-c756efb3befc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!apt install -y tesseract-ocr tesseract-ocr-fra\n",
        "!pip install pytesseract pdf2image pillow\n",
        "!apt install -y poppler-utils\n",
        "!pip install NTLK\n",
        "!pip install pymupdf\n"
      ],
      "metadata": {
        "id": "92LvaGM3m64M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "572cda22-3d22-4c76-e4dd-eacbbc3a059b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.3.2 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-fra tesseract-ocr-osd\n",
            "0 upgraded, 4 newly installed, 0 to remove and 23 not upgraded.\n",
            "Need to get 5,344 kB of archives.\n",
            "After this operation, 16.8 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-fra all 1:4.00~git30-7274cfa-1.1 [527 kB]\n",
            "Fetched 5,344 kB in 1s (3,566 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 124947 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Selecting previously unselected package tesseract-ocr-fra.\n",
            "Preparing to unpack .../tesseract-ocr-fra_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-fra (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-fra (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pytesseract, pdf2image\n",
            "Successfully installed pdf2image-1.17.0 pytesseract-0.3.13\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 23 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.6 [186 kB]\n",
            "Fetched 186 kB in 1s (266 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 124998 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.6_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.6) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.6) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement NTLK (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for NTLK\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytesseract\n",
        "from pdf2image import convert_from_path"
      ],
      "metadata": {
        "id": "SCbf339oaU6D"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extraction des donnee\n",
        "\n",
        "Ce code permet d'extraire du texte d'un document PDF en utilisant la reconnaissance optique de caractères (OCR) grâce à Tesseract. Il commence par convertir chaque page du PDF en image, puis applique l'OCR sur ces images pour récupérer le texte. Ce texte est ensuite stocké dans un fichier afin d’être facilement exploitable. Ensuite, la fonction lire_fichier est utilisée pour lire le contenu d'un fichier texte. Elle prend en entrée le chemin du fichier, tente de l'ouvrir en mode lecture avec un encodage UTF-8, et retourne son contenu. Si le fichier est introuvable, un message d'erreur est affiché, et la fonction retourne None.  Si le fichier est correctement lu, son contenu est retourné pour être utilisé ou affiché.\n",
        "\n"
      ],
      "metadata": {
        "id": "BFZCGjAg1I0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive_path=\"/content/drive/MyDrive\"\n",
        "\n",
        "pdf_path1 = f\"{drive_path}/Test/Bail 1.pdf\"\n",
        "\n",
        "pdf_path2 = f\"{drive_path}/Test/Bail 2.pdf\""
      ],
      "metadata": {
        "id": "xrDQ47nIahEs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    # Ouvre le fichier PDF\n",
        "    document = fitz.open(pdf_path)\n",
        "\n",
        "    text = \"\"\n",
        "\n",
        "    # Parcours de toutes les pages du PDF\n",
        "    for page_num in range(len(document)):\n",
        "        page = document.load_page(page_num)  # Charge la page\n",
        "        text += page.get_text()  # Ajoute le texte de la page à la variable 'text'\n",
        "\n",
        "    return text\n",
        "\n"
      ],
      "metadata": {
        "id": "Wp6zicdC-9OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import pytesseract\n",
        "\n",
        "def extract_text_from_image(pdf_path, output_file):\n",
        "# Convertir le PDF en images\n",
        "  images = convert_from_path(pdf_path)\n",
        "\n",
        "# Extraire le texte avec Tesseract OCR\n",
        "  text = \"\"\n",
        "  for img in images:\n",
        "    text += pytesseract.image_to_string(img, lang=\"fra\") + \"\\n\\n\"\n",
        "\n",
        "# Sauvegarde le texte extrait\n",
        "  with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(text)\n",
        "\n",
        "  print(f\"Le texte a été extrait et sauvegardé dans {output_file}\")\n",
        "  return output_file"
      ],
      "metadata": {
        "id": "b_WwoW1UyPqC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(extract_text_from_image(pdf_path1,f\"{drive_path}/Test/Bail 1.txt\"))\n",
        "extract_text_from_image(pdf_path2,f\"{drive_path}/Test/Bail 2.txt\")"
      ],
      "metadata": {
        "id": "tW2P5Jz34T1X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "2eab967e-93d8-4197-fe22-77827fd121e9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le texte a été extrait et sauvegardé dans /content/drive/MyDrive/Test/Bail 1.txt\n",
            "/content/drive/MyDrive/Test/Bail 1.txt\n",
            "Le texte a été extrait et sauvegardé dans /content/drive/MyDrive/Test/Bail 2.txt\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Test/Bail 2.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lire_fichier(chemin_fichier):\n",
        "    try:\n",
        "        with open(chemin_fichier, \"r\", encoding=\"utf-8\") as fichier:\n",
        "            contenu = fichier.read()\n",
        "        return contenu\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Erreur : Le fichier '{chemin_fichier}' n'existe pas.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur lors de la lecture du fichier : {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "_w4ewumWCOXO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = lire_fichier(f\"{drive_path}/Test/Bail 1.txt\")\n",
        "text2 = lire_fichier(f\"{drive_path}/Test/Bail 2.txt\")\n"
      ],
      "metadata": {
        "id": "9ZYnXboRCeJI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_words_with_indices(text):\n",
        "    words = []\n",
        "    for match in re.finditer(r'\\S+', text):  # \\S+ capture les mots sans espaces\n",
        "        word = match.group()\n",
        "        start_idx = match.start()\n",
        "        end_idx = match.end()\n",
        "        words.append((word, start_idx, end_idx))\n",
        "    return words\n",
        "\n",
        "# Exemple d'utilisation"
      ],
      "metadata": {
        "id": "QPCeH6aTbBOZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = extract_words_with_indices(text1)\n",
        "print(result)\n",
        "result2 = extract_words_with_indices(text2)\n",
        "#print(result2)"
      ],
      "metadata": {
        "id": "bKsVlQuIHyOg",
        "outputId": "1f3c68ce-835d-4f61-9497-07a18e692e79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Bail', 0, 4), ('1', 5, 6), ('BAIL', 8, 12), ('COMMERCIAL', 13, 23), ('ENTRE', 24, 29), ('LES', 30, 33), ('SOUSSIGNES', 34, 44), (':', 45, 46), ('Le', 47, 49), ('Bailleur', 50, 58), (':', 59, 60), ('Monsieur', 62, 70), ('Bernard', 71, 78), ('Lefevre,', 79, 87), ('demeurant', 88, 97), ('à', 98, 99), ('15', 100, 102), ('Rue', 103, 106), ('de', 107, 109), ('la', 110, 112), ('Paix,', 113, 118), ('75002', 119, 124), ('Paris,', 125, 131), ('téléphone', 132, 141), (':', 142, 143), ('01', 144, 146), ('45', 147, 149), ('67', 150, 152), ('89', 153, 155), ('10,', 156, 159), ('email', 160, 165), (':', 166, 167), ('bernard.lefevre@email.com,', 168, 194), ('ci-après', 195, 203), ('désigné', 204, 211), ('\"Le', 212, 215), ('Bailleur\".', 216, 226), ('Le', 227, 229), ('Preneur', 230, 237), (':', 238, 239), ('Société', 241, 248), ('\"Les', 249, 253), ('Vêtements', 254, 263), ('de', 264, 266), ('Paris\",', 267, 274), ('représentée', 275, 286), ('par', 287, 290), ('Monsieur', 291, 299), ('François', 300, 308), ('Dupont,', 309, 316), ('Directeur,', 317, 327), ('ayant', 328, 333), ('son', 334, 337), ('siège', 338, 343), ('social', 344, 350), ('à', 351, 352), ('10', 353, 355), ('Rue', 356, 359), ('du', 360, 362), ('Faubourg', 363, 371), ('Saint-Antoine,', 372, 386), ('75011', 387, 392), ('Paris,', 393, 399), ('téléphone', 400, 409), (':', 410, 411), ('01', 412, 414), ('45', 415, 417), ('67', 418, 420), ('89', 421, 423), ('11,', 424, 427), ('email', 428, 433), (':', 434, 435), ('contact@vetementsparis.com,', 436, 463), ('ci-après', 464, 472), ('désignée', 473, 481), ('\"Le', 482, 485), ('Preneur\".', 486, 495), ('DESIGNATION', 497, 508), ('DES', 509, 512), ('LIEUX', 513, 518), ('LOUEES', 519, 525), ('A', 527, 528), ('Paris,', 529, 535), ('10', 536, 538), ('Rue', 539, 542), ('du', 543, 545), ('Faubourg', 546, 554), ('Saint-Antoine,', 555, 569), ('au', 570, 572), ('rez-de-chaussée', 573, 588), (\"d'un\", 589, 593), ('immeuble,', 594, 603), ('un', 604, 606), ('magasin', 607, 614), ('de', 615, 617), ('120', 618, 621), ('m?', 622, 624), ('composé', 625, 632), ('de', 633, 635), ('deux', 636, 640), ('salles', 641, 647), ('à', 648, 649), ('usage', 650, 655), ('commercial,', 656, 667), ('une', 668, 671), ('petite', 672, 678), ('cuisine,', 679, 687), ('et', 688, 690), ('une', 691, 694), ('salle', 695, 700), (\"d'eau.\", 701, 707), ('DUREE', 709, 714), ('Le', 716, 718), ('présent', 719, 726), ('bail', 727, 731), ('est', 732, 735), ('consenti', 736, 744), ('et', 745, 747), ('accepté', 748, 755), ('pour', 756, 760), ('une', 761, 764), ('durée', 765, 770), ('de', 771, 773), ('5', 774, 775), ('années', 776, 782), ('consécutives,', 783, 796), ('à', 797, 798), ('compter', 799, 806), ('du', 807, 809), ('01', 810, 812), ('janvier', 813, 820), ('2025.', 821, 826), ('Le', 827, 829), ('bail', 830, 834), ('prendra', 835, 842), ('fin', 843, 846), ('le', 847, 849), ('31', 850, 852), ('décembre', 853, 861), ('2030,', 862, 867), ('sauf', 868, 872), ('résiliation', 873, 884), ('par', 885, 888), (\"l'une\", 889, 894), ('des', 895, 898), ('parties,', 899, 907), ('conformément', 908, 920), ('aux', 921, 924), ('dispositions', 925, 937), ('prévues', 938, 945), ('par', 946, 949), ('le', 950, 952), ('code', 953, 957), ('civil.', 958, 964), ('Le', 966, 968), ('preneur', 969, 976), ('pourra', 977, 983), ('demander', 984, 992), ('une', 993, 996), ('reconduction', 997, 1009), ('tacite', 1010, 1016), ('du', 1017, 1019), ('bail', 1020, 1024), ('à', 1025, 1026), ('son', 1027, 1030), ('terme,', 1031, 1037), ('sauf', 1038, 1042), ('avis', 1043, 1047), ('contraire', 1048, 1057), ('écrit', 1058, 1063), ('du', 1064, 1066), ('bailleur,', 1067, 1076), ('au', 1077, 1079), ('moins', 1080, 1085), ('trois', 1086, 1091), ('mois', 1092, 1096), ('avant', 1097, 1102), ('la', 1103, 1105), ('fin', 1106, 1109), ('du', 1110, 1112), ('contrat.', 1113, 1121), ('DESTINATION', 1123, 1134), ('DES', 1135, 1138), ('LIEUX', 1139, 1144), ('Le', 1146, 1148), ('local', 1149, 1154), ('ci-dessus', 1155, 1164), ('sera', 1165, 1169), ('loué', 1170, 1174), ('à', 1175, 1176), ('usage', 1177, 1182), ('de', 1183, 1185), ('magasin', 1186, 1193), ('de', 1194, 1196), ('vêtements', 1197, 1206), ('pour', 1207, 1211), ('homme,', 1212, 1218), ('et', 1219, 1221), ('activités', 1222, 1231), ('annexes.', 1232, 1240), ('Il', 1241, 1243), ('est', 1244, 1247), ('expressément', 1248, 1260), ('interdit', 1261, 1269), (\"d'exercer\", 1270, 1279), ('toute', 1280, 1285), ('autre', 1286, 1291), ('activité', 1292, 1300), ('sans', 1301, 1305), (\"l'accord\", 1306, 1314), ('écrit', 1315, 1320), ('du', 1321, 1323), ('bailleur.', 1324, 1333), ('CONDITIONS', 1335, 1345), ('Le', 1347, 1349), ('présent', 1350, 1357), ('bail', 1358, 1362), ('est', 1363, 1366), ('consenti', 1367, 1375), ('et', 1376, 1378), ('accepté', 1379, 1386), ('sous', 1387, 1391), ('les', 1392, 1395), ('clauses', 1396, 1403), ('et', 1404, 1406), ('conditions', 1407, 1417), ('suivantes', 1418, 1427), ('que', 1428, 1431), ('le', 1432, 1434), ('preneur', 1435, 1442), (\"s'engage\", 1443, 1451), ('à', 1452, 1453), ('exécuter', 1454, 1462), (':', 1463, 1464), ('1.', 1466, 1468), ('Jouir', 1469, 1474), ('des', 1475, 1478), ('lieux', 1479, 1484), ('loués', 1485, 1490), ('dans', 1491, 1495), (\"l'état\", 1496, 1502), ('où', 1503, 1505), ('ils', 1506, 1509), ('se', 1510, 1512), ('trouvent,', 1513, 1522), ('sans', 1523, 1527), ('pouvoir', 1528, 1535), ('exercer', 1536, 1543), ('aucun', 1544, 1549), ('recours', 1550, 1557), ('contre', 1559, 1565), ('le', 1566, 1568), ('bailleur', 1569, 1577), ('pour', 1578, 1582), ('toute', 1583, 1588), ('défectuosité', 1589, 1601), ('existante.', 1602, 1612), ('2.', 1614, 1616), ('Jouir', 1617, 1622), ('de', 1623, 1625), (\"l'ensemble\", 1626, 1636), ('des', 1637, 1640), ('locaux', 1641, 1647), ('sans', 1648, 1652), ('nuire', 1653, 1658), ('à', 1659, 1660), ('la', 1661, 1663), ('tranquillité', 1664, 1676), ('du', 1677, 1679), ('voisinage', 1680, 1689), ('et', 1690, 1692), ('sans', 1693, 1697), ('y', 1698, 1699), ('commettre', 1700, 1709), ('aucun', 1710, 1715), ('abus.', 1716, 1721)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# fine-Tuning"
      ],
      "metadata": {
        "id": "rNYBLef33glH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "def tokenize_sentence_french(sentence):\n",
        "    # Tokenisation avec NLTK pour le français\n",
        "    tokens = nltk.word_tokenize(sentence, language='french')\n",
        "    return tokens\n",
        "\n",
        "# Exemple d'utilisation"
      ],
      "metadata": {
        "id": "9a5tUczs-pRt",
        "outputId": "6bc207d4-6069-4a83-f4d4-fd3fb7226b1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creation du Dataset\n",
        "Ce code prépare des données pour entraîner un modèle de reconnaissance d'entités nommées (NER) en utilisant Camembert pour effectuer un fine-tuning sur des textes annotés. Il commence par définir des annotations sous forme de textes et leurs entités correspondantes. Ensuite, il convertit ces annotations en format IOB (Inside, Outside, Beginning), qui est un format standard utilisé pour les tâches de NER. Le code crée ensuite un dataset Hugging Face en utilisant ces annotations et étiquettes.\n",
        "\n",
        "Le modèle Camembert (de type CamembertForTokenClassification) est ensuite chargé, ainsi que son tokenizer. Les tokens sont traités avec le tokenizer Camembert et les étiquettes sont alignées avec les tokens correspondants.\n",
        "\n",
        "Enfin, le dataset est préparé pour l’entraînement en appliquant la fonction de tokenisation et d'alignement sur toutes les entrées du dataset."
      ],
      "metadata": {
        "id": "b9CFn1vF3oWm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LIZ4KNhgm-UJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "7fFnLZHamaGT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "278d1edba9794abdbb34a3bd1783f758",
            "90e02a8549c741b89e9ace3fcea423e6",
            "3329b21f272d471d824cc75c0ca17681",
            "7a509a698deb4bf498cdf454c53c9b9c",
            "7b4c6f27589d4d2b909c8bfa6c34a314",
            "a6aafe80177c471395de7fc7218caccd",
            "c23c670538bf4209ba8246fcc34b7ae3",
            "179f5fbac77e4fd59a5ecf5e7fb51329",
            "cabbd64074c342818fa60158bc27b0d0",
            "144e115d3c2b4aa9b53eca9dd499159f",
            "62c0f3c61b634044abfe68f90e7b16bb"
          ]
        },
        "outputId": "92b738a2-8250-4a72-ffa8-656b04709307"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'tokens': ['Bail', '1', 'BAIL', 'COMMERCIAL', 'ENTRE', 'LES', 'SOUSSIGNES', ':', 'Le', 'Bailleur', ':', 'Monsieur', 'Bernard', 'Lefevre', ',', 'demeurant', 'à', '15', 'Rue', 'de', 'la', 'Paix', ',', '75002', 'Paris', ',', 'téléphone', ':', '01', '45', '67', '89', '10', ',', 'email', ':', 'bernard.lefevre', '@', 'email.com', ',', 'ci-après', 'désigné', '``', 'Le', 'Bailleur', \"''\", '.', 'Le', 'Preneur', ':', 'Société', '``', 'Les', 'Vêtements', 'de', 'Paris', \"''\", ',', 'représentée', 'par', 'Monsieur', 'François', 'Dupont', ',', 'Directeur', ',', 'ayant', 'son', 'siège', 'social', 'à', '10', 'Rue', 'du', 'Faubourg', 'Saint-Antoine', ',', '75011', 'Paris', ',', 'téléphone', ':', '01', '45', '67', '89', '11', ',', 'email', ':', 'contact', '@', 'vetementsparis.com', ',', 'ci-après', 'désignée', '``', 'Le', 'Preneur', \"''\", '.', 'DESIGNATION', 'DES', 'LIEUX', 'LOUEES', 'A', 'Paris', ',', '10', 'Rue', 'du', 'Faubourg', 'Saint-Antoine', ',', 'au', 'rez-de-chaussée', \"d'un\", 'immeuble', ',', 'un', 'magasin', 'de', '120', 'm', '?', 'composé', 'de', 'deux', 'salles', 'à', 'usage', 'commercial', ',', 'une', 'petite', 'cuisine', ',', 'et', 'une', 'salle', \"d'eau\", '.', 'DUREE', 'Le', 'présent', 'bail', 'est', 'consenti', 'et', 'accepté', 'pour', 'une', 'durée', 'de', '5', 'années', 'consécutives', ',', 'à', 'compter', 'du', '01', 'janvier', '2025', '.', 'Le', 'bail', 'prendra', 'fin', 'le', '31', 'décembre', '2030', ',', 'sauf', 'résiliation', 'par', \"l'une\", 'des', 'parties', ',', 'conformément', 'aux', 'dispositions', 'prévues', 'par', 'le', 'code', 'civil', '.', 'Le', 'preneur', 'pourra', 'demander', 'une', 'reconduction', 'tacite', 'du', 'bail', 'à', 'son', 'terme', ',', 'sauf', 'avis', 'contraire', 'écrit', 'du', 'bailleur', ',', 'au', 'moins', 'trois', 'mois', 'avant', 'la', 'fin', 'du', 'contrat', '.', 'DESTINATION', 'DES', 'LIEUX', 'Le', 'local', 'ci-dessus', 'sera', 'loué', 'à', 'usage', 'de', 'magasin', 'de', 'vêtements', 'pour', 'homme', ',', 'et', 'activités', 'annexes', '.', 'Il', 'est', 'expressément', 'interdit', \"d'exercer\", 'toute', 'autre', 'activité', 'sans', \"l'accord\", 'écrit', 'du', 'bailleur', '.', 'CONDITIONS', 'Le', 'présent', 'bail', 'est', 'consenti', 'et', 'accepté', 'sous', 'les', 'clauses', 'et', 'conditions', 'suivantes', 'que', 'le', 'preneur', \"s'engage\", 'à', 'exécuter', ':', '1', '.', 'Jouir', 'des', 'lieux', 'loués', 'dans', \"l'état\", 'où', 'ils', 'se', 'trouvent', ',', 'sans', 'pouvoir', 'exercer', 'aucun', 'recours', 'contre', 'le', 'bailleur', 'pour', 'toute', 'défectuosité', 'existante', '.', '2', '.', 'Jouir', 'de', \"l'ensemble\", 'des', 'locaux', 'sans', 'nuire', 'à', 'la', 'tranquillité', 'du', 'voisinage', 'et', 'sans', 'y', 'commettre', 'aucun', 'abus', '.'], 'labels': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-BAIL', 'I-BAIL', 'O', 'O', 'O', 'B-ADDRB', 'I-ADDRB', 'I-ADDRB', 'I-ADDRB', 'I-ADDRB', 'I-ADDRB', 'I-ADDRB', 'I-ADDRB', 'O', 'O', 'O', 'B-NUMB', 'I-NUMB', 'I-NUMB', 'I-NUMB', 'I-NUMB', 'O', 'O', 'O', 'B-MAILB', 'I-MAILB', 'I-MAILB', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PREN', 'I-PREN', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ADDRP', 'I-ADDRP', 'I-ADDRP', 'I-ADDRP', 'I-ADDRP', 'I-ADDRP', 'I-ADDRP', 'I-ADDRP', 'O', 'O', 'O', 'B-NUMP', 'I-NUMP', 'I-NUMP', 'I-NUMP', 'I-NUMP', 'O', 'O', 'O', 'B-MAILP', 'I-MAILP', 'I-MAILP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DES', 'O', 'B-SURF', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DURE', 'I-DURE', 'O', 'O', 'O', 'O', 'O', 'B-DPE', 'I-DPE', 'I-DPE', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FDB', 'I-FDB', 'I-FDB', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}, {'tokens': ['Bail', '2', 'CONTRAT', 'DE', 'BAIL', 'COMMERCIAL', 'ENTRE', 'LES', 'SOUSSIGNES', ':', 'Le', 'Bailleur', ':', 'Monsieur', 'Pierre', 'Durand', ',', 'demeurant', 'à', '25', 'Rue', 'de', 'la', 'République', ',', '75003', 'Paris', ',', 'téléphone', ':', '01', '23', '45', '67', '89', ',', 'email', ':', 'pierre.durand', '@', 'email.com', ',', 'ci-après', 'dénommé', '``', 'Le', 'Bailleur', \"''\", '.', 'Le', 'Preneur', ':', 'La', 'Société', '``', 'Le', 'Petit', 'Bistrot', \"''\", ',', 'représentée', 'par', 'Monsieur', 'Jacques', 'Lefevre', ',', 'gérant', ',', 'ayant', 'son', 'siège', 'social', 'à', '5', 'Rue', 'du', 'Marché', ',', '75005', 'Paris', ',', 'téléphone', ':', '01', '45', '67', '89', '10', ',', 'email', ':', 'contact', '@', 'lepetitbistro', 't.com', ',', 'ci-après', 'dénommée', '``', 'Le', 'Preneur', \"''\", '.', 'DESIGNATION', 'DES', 'LIEUX', 'LOUEES', 'Le', 'Bailleur', 'consent', 'par', 'le', 'présent', 'contrat', 'à', 'louer', 'au', 'Preneur', ',', 'qui', 'accepte', ',', 'le', 'local', 'commercial', 'situé', 'à', '7', 'Rue', 'de', 'la', 'Gare', ',', '75012', 'Paris', ',', \"d'une\", 'superficie', 'de', '100', 'm', '?', ',', 'comprenant', 'une', 'salle', 'principale', ',', 'une', 'réserve', ',', 'et', 'des', 'sanitaires', '.', 'Le', 'bien', 'est', 'destiné', 'exclusivement', 'à', \"l'exploitation\", \"d'un\", 'restaurant', 'et', 'toute', 'autre', 'activité', 'est', 'expressément', 'interdite', 'sans', \"l'accord\", 'préalable', 'du', 'Bailleur', '.', 'DUREE', 'Le', 'présent', 'bail', 'est', 'consenti', 'pour', 'une', 'durée', 'de', '6', 'années', ',', 'à', 'compter', 'du', '1er', 'avril', '2025', 'et', 'prendra', 'fin', 'le', '31', 'mars', '2031', '.', 'À', \"l'issue\", 'de', 'cette', 'période', ',', 'le', 'contrat', 'pourra', 'être', 'renouvelé', 'tacitement', 'pour', 'une', 'durée', 'identique', ',', 'sauf', 'si', \"l'une\", 'des', 'parties', 'notifie', 'son', 'intention', 'de', 'ne', 'pas', 'renouveler', 'le', 'bail', ',', 'au', 'moins', 'trois', 'mois', 'avant', \"l'expiration\", 'du', 'contrat', 'en', 'cours', '.', 'LOYER', 'ET', 'CHARGES', 'Le', 'montant', 'du', 'loyer', 'annuel', 'est', 'fixé', 'à', '24', '000', '€', ',', 'payable', 'mensuellement', 'par', 'virement', 'bancaire', 'au', 'compte', 'du', 'Bailleur', '.', 'Ce', 'montant', 'est', 'révisable', 'chaque', 'année', ',', 'conformément', 'aux', 'indices', 'de', \"l'INSEE\", '.', 'Le', 'Preneur', \"s'engage\", 'également', 'à', 'payer', 'toutes', 'les', 'charges', 'relatives', 'à', \"l'entretien\", 'et', 'au', 'fonctionnement', 'des', 'lieux', 'loués', ',', 'incluant', 'les', 'taxes', 'foncières', 'et', 'la', 'consommation', \"d'eau\", 'et', \"d'électricité\", '.', 'DESTINATION', 'DES', 'LIEUX', 'Le', 'local', 'est', 'exclusivement', 'destiné', 'à', \"l'exploitation\", \"d'un\", 'restaurant', ',', 'incluant', 'la', 'vente', 'de', 'nourriture', 'et', 'de', 'boissons', '.', 'Le', 'Preneur', \"s'engage\", 'à', 'respecter', 'la', 'réglementation', 'en', 'vigueur', 'relative', 'à', \"l'hygiène\", 'et', 'à', 'la', 'sécurité', '.', 'CONDITIONS', 'GÉNÉRALES', 'Le', 'Preneur', \"s'engage\", 'à', ':', '1', '.', 'Maintenir', 'les', 'lieux', 'dans', 'un', 'bon', 'état', 'et', 'à', 'effectuer', 'toutes', 'les', 'réparations', 'nécessaires', '.', '2', '.', 'Ne', 'pas', 'sous-louer', 'le', 'bien', 'sans', \"l'accord\", 'préalable', 'du', 'Bailleur', '.', '3', '.', 'Respecter', 'le', 'règlement', 'de', 'copropriété', ',', 'ainsi', 'que', 'toutes', 'les', 'lois', 'et', 'réglementations', 'en', 'vigueur', 'concernant', \"l'exploitation\", \"d'un\", 'restaurant', '.', 'Le', 'Bailleur', ',', 'de', 'son', 'côté', ',', \"s'engage\", 'à', 'fournir', 'au', 'Preneur', 'un', 'local', 'conforme', 'aux', 'normes', 'de', 'sécurité', 'et', \"d'hygiène\", 'et', 'à', 'ne', 'pas', 'nuire', 'à', \"l'exploitation\", 'du', 'restaurant', 'pendant', 'toute', 'la', 'durée', 'du', 'bail', '.'], 'labels': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-BAIL', 'I-BAIL', 'O', 'O', 'O', 'B-ADDRB', 'I-ADDRB', 'I-ADDRB', 'I-ADDRB', 'I-ADDRB', 'I-ADDRB', 'I-ADDRB', 'I-ADDRB', 'O', 'O', 'O', 'B-NUMB', 'I-NUMB', 'I-NUMB', 'I-NUMB', 'I-NUMB', 'O', 'O', 'O', 'B-MAILB', 'I-MAILB', 'I-MAILB', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PREN', 'I-PREN', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ADDRP', 'I-ADDRP', 'I-ADDRP', 'I-ADDRP', 'I-ADDRP', 'I-ADDRP', 'I-ADDRP', 'O', 'O', 'O', 'B-NUMP', 'I-NUMP', 'I-NUMP', 'I-NUMP', 'I-NUMP', 'O', 'O', 'O', 'B-MAILP', 'I-MAILP', 'I-MAILP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-SURF', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DES', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DURE', 'I-DURE', 'O', 'O', 'O', 'O', 'B-DPE', 'I-DPE', 'I-DPE', 'O', 'O', 'O', 'O', 'B-FDB', 'I-FDB', 'I-FDB', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}]\n",
            "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 16, 0, 0, 0, 1, 14, 14, 14, 14, 14, 14, 14, 0, 0, 0, 10, 22, 22, 22, 22, 0, 0, 0, 8, 20, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 24, 0, 0, 0, 0, 0, 0, 0, 0, 2, 15, 15, 15, 15, 15, 15, 15, 0, 0, 0, 11, 23, 23, 23, 23, 0, 0, 0, 9, 21, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 18, 0, 0, 0, 0, 0, 5, 17, 17, 0, 0, 0, 0, 0, 0, 7, 19, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 16, 0, 0, 0, 1, 14, 14, 14, 14, 14, 14, 14, 0, 0, 0, 10, 22, 22, 22, 22, 0, 0, 0, 8, 20, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 24, 0, 0, 0, 0, 0, 0, 0, 0, 2, 15, 15, 15, 15, 15, 15, 0, 0, 0, 11, 23, 23, 23, 23, 0, 0, 0, 9, 21, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 18, 0, 0, 0, 0, 5, 17, 17, 0, 0, 0, 0, 7, 19, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of CamembertForTokenClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "278d1edba9794abdbb34a3bd1783f758"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[None, 0, 0, 1, 2, 2, 3, 3, 3, 3, 4, 4, 5, 6, 6, 6, 6, 7, 8, 9, 9, 10, 11, 12, 13, 13, 13, 14, 14, 15, 16, 17, 18, 19, 20, 21, 22, 22, 23, 23, 24, 25, 25, 26, 27, 28, 29, 30, 31, 32, 33, 33, 34, 35, 36, 36, 36, 36, 36, 36, 37, 38, 38, 38, 39, 39, 40, 40, 40, 41, 42, 42, 43, 44, 44, 45, 46, 46, 47, 48, 48, 49, 50, 51, 51, 52, 53, 54, 55, 56, 57, 57, 58, 59, 60, 61, 62, 63, 63, 64, 65, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 74, 75, 75, 75, 76, 76, 77, 77, 78, 79, 79, 80, 81, 82, 83, 84, 85, 86, 87, 87, 88, 89, 90, 91, 92, 92, 92, 92, 92, 92, 93, 93, 94, 94, 94, 95, 96, 96, 97, 98, 98, 99, 100, 100, 101, 101, 101, 102, 103, 103, 104, 104, 104, 105, 106, 107, 107, 108, 109, 110, 111, 111, 112, 112, 112, 113, 113, 114, 115, 115, 115, 115, 115, 116, 116, 116, 117, 118, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 132, 133, 134, 135, 136, 136, 137, 138, 139, 140, 140, 140, 141, 141, 142, 142, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 156, 157, 157, 158, 159, 160, 161, 162, 163, 163, 164, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 173, 174, 175, 176, 177, 177, 177, 178, 179, 180, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 189, 190, 191, 192, 193, 194, 195, 195, 196, 197, 198, 199, 200, 201, 202, 202, 203, 204, 205, 206, 207, 208, 209, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 219, 220, 220, 220, 221, 222, 222, 223, 224, 225, 225, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 236, 237, 238, 239, 240, 240, 241, 242, 243, 244, 245, 245, 245, 246, 247, 248, 249, 250, 250, 250, 251, 252, 253, 254, 254, 255, 255, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 265, 266, 267, 268, 269, 270, 271, 272, 272, 272, 273, 274, 275, 276, 277, 277, 278, 278, 279, 280, 281, 281, 282, 283, 283, 283, 284, 285, 286, 287, 288, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 299, 299, 299, 300, 301, 301, 302, 303, 303, 304, 304, 305, 306, 306, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 322, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
            "[None, 0, 0, 1, 2, 2, 3, 4, 4, 5, 5, 5, 5, 6, 6, 7, 8, 8, 8, 8, 9, 10, 11, 11, 12, 13, 14, 15, 16, 16, 17, 18, 19, 20, 21, 22, 23, 24, 24, 25, 25, 26, 27, 27, 28, 29, 30, 31, 32, 33, 34, 35, 35, 36, 37, 38, 38, 38, 38, 39, 40, 40, 40, 41, 41, 42, 42, 42, 43, 44, 44, 45, 46, 46, 47, 48, 48, 49, 50, 50, 51, 52, 53, 54, 54, 55, 56, 57, 57, 57, 58, 59, 59, 60, 61, 62, 63, 64, 64, 64, 65, 65, 66, 67, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 77, 78, 78, 79, 80, 80, 81, 82, 83, 84, 85, 86, 87, 88, 88, 89, 90, 91, 92, 93, 93, 93, 93, 94, 94, 94, 95, 95, 96, 96, 96, 97, 97, 98, 98, 99, 100, 100, 101, 102, 102, 103, 103, 103, 104, 105, 105, 106, 106, 106, 107, 108, 108, 109, 109, 110, 111, 112, 113, 114, 115, 116, 117, 117, 118, 118, 119, 120, 121, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 132, 133, 133, 134, 135, 135, 136, 136, 136, 137, 138, 139, 140, 141, 142, 142, 143, 144, 145, 146, 147, 147, 148, 149, 150, 150, 151, 152, 153, 154, 154, 155, 156, 157, 158, 159, 160, 161, 161, 161, 162, 162, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 172, 172, 173, 174, 175, 175, 176, 176, 177, 177, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 189, 190, 191, 192, 193, 193, 194, 195, 195, 196, 197, 198, 199, 200, 201, 202, 202, 203, 203, 204, 205, 205, 205, 206, 207, 208, 209, 209, 210, 211, 212, 213, 214, 215, 215, 216, 217, 218, 219, 220, 220, 221, 222, 223, 223, 223, 224, 225, 226, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 235, 236, 237, 238, 239, 240, 241, 241, 241, 242, 243, 244, 245, 246, 246, 247, 247, 247, 248, 249, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 261, 262, 262, 263, 263, 264, 265, 266, 267, 268, 269, 270, 270, 271, 271, 272, 273, 274, 275, 275, 275, 276, 277, 278, 278, 279, 280, 281, 282, 283, 283, 283, 284, 284, 285, 286, 286, 287, 287, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 296, 296, 297, 298, 299, 300, 301, 302, 302, 303, 303, 304, 305, 306, 307, 307, 308, 309, 310, 311, 311, 311, 312, 313, 313, 313, 314, 314, 315, 315, 315, 316, 317, 317, 318, 319, 320, 321, 322, 323, 324, 324, 324, 325, 325, 325, 326, 327, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 336, 337, 338, 338, 339, 339, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 348, 348, 349, 350, 351, 352, 353, 353, 354, 354, 354, 355, 355, 355, 355, 355, 355, 356, 357, 357, 358, 358, 358, 359, None]\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import CamembertForTokenClassification, CamembertTokenizer, TrainingArguments, Trainer\n",
        "\n",
        "# Exemple d'annotations\n",
        "annotations = [\n",
        "\n",
        "    {\"text\":text1 , \"entities\":[(71,86, \"BAIL\") , (100, 130, \"ADDRB\") , (144, 158, \"NUMB\") , (168, 193, \"MAILB\") , (300, 315, \"PREN\"),(353, 398, \"ADDRP\"),(412, 426, \"NUMP\"),(436, 462, \"MAILP\"),(607, 614, \"DES\"),(618, 621, \"SURF\"),(774, 782, \"DURE\"),(810, 825, \"DPE\"),(850, 866, \"FDB\")]},\n",
        "    {\"text\":text2 , \"entities\":[(82, 95, \"BAIL\") , (109, 145, \"ADDRB\") , (159, 173, \"NUMB\"),(183, 206, \"MAILB\") , (310, 325, \"PREN\"),(361, 389, \"ADDRP\"),(403, 417, \"NUMP\"),(427, 448, \"MAILP\"),(674, 677, \"SURF\"),(804, 814, \"DES\"),(958, 966, \"DURE\"),(981, 995, \"DPE\"),(1014, 1026, \"FDB\")]},\n",
        "\n",
        "]\n",
        "\n",
        "import re\n",
        "\n",
        "def convert_to_iob(annotation):\n",
        "    text_cleaned = annotation[\"text\"]\n",
        "    tokens = tokenize_sentence_french(text_cleaned)\n",
        "    # Initialisation des labels IOB\n",
        "    labels = [\"O\"] * len(tokens)\n",
        "\n",
        "    for start, end, label in annotation[\"entities\"]:\n",
        "        entity_text = annotation[\"text\"][start:end]\n",
        "\n",
        "        entity_tokens = tokenize_sentence_french(entity_text)\n",
        "\n",
        "        # Trouver le premier index correspondant à l'entité\n",
        "        try:\n",
        "            nb = len(tokenize_sentence_french(annotation[\"text\"][:start]))\n",
        "            start_token_idx = tokenize_sentence_french(annotation[\"text\"][start:]).index(entity_tokens[0])+nb\n",
        "        except ValueError:\n",
        "            print(f\"⚠️ Entité '{entity_text}' non trouvée dans les tokens : {tokens[:50]}\")\n",
        "            continue\n",
        "        for i in range(len(entity_tokens)):\n",
        "            if start_token_idx + i < len(labels):  # Vérification pour éviter les dépassements\n",
        "                labels[start_token_idx + i] = f\"B-{label}\" if i == 0 else f\"I-{label}\"\n",
        "\n",
        "    return {\"tokens\": tokens, \"labels\": labels}\n",
        "\n",
        "\n",
        "# Création du dataset\n",
        "train_data = [convert_to_iob(ann) for ann in annotations]\n",
        "print(train_data)\n",
        "\n",
        "# Définition des labels\n",
        "unique_labels = set(label for entry in train_data for label in entry[\"labels\"] if label != \"O\")\n",
        "label2id = {label: i for i, label in enumerate([\"O\"] + sorted(unique_labels))}\n",
        "id2label = {i: label for label, i in label2id.items()}\n",
        "\n",
        "# Transformer en dataset Hugging Face\n",
        "train_dataset = Dataset.from_dict({\n",
        "    \"tokens\": [entry[\"tokens\"] for entry in train_data],\n",
        "    \"labels\": [[label2id[label] for label in entry[\"labels\"]] for entry in train_data]\n",
        "})\n",
        "print(  [[label2id[label] for label in entry[\"labels\"]] for entry in train_data])\n",
        "\n",
        "# Charger le tokenizer et le modèle Camembert\n",
        "from transformers import CamembertTokenizerFast\n",
        "\n",
        "# Charger le tokenizer rapide\n",
        "tokenizer = CamembertTokenizerFast.from_pretrained(\"camembert-base\")\n",
        "model = CamembertForTokenClassification.from_pretrained(\"camembert-base\", num_labels=len(label2id), id2label=id2label, label2id=label2id)\n",
        "\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, padding=True, is_split_into_words=True)\n",
        "    labels = examples[\"labels\"]\n",
        "    aligned_labels = []\n",
        "    for i, label in enumerate(labels):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        print(word_ids)\n",
        "\n",
        "        aligned_label = []\n",
        "        prev_word_idx = None\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                aligned_label.append(-100)  # Ignore token\n",
        "            elif word_idx != prev_word_idx:\n",
        "                aligned_label.append(label[word_idx])  # Premier token du mot\n",
        "            else:\n",
        "                aligned_label.append(label[word_idx])  # Autres tokens du mot\n",
        "            prev_word_idx = word_idx\n",
        "        aligned_labels.append(aligned_label)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = aligned_labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "# Appliquer la tokenisation\n",
        "train_dataset = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
        "\n",
        "\n",
        "# Afficher les premières données tokenisées"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce code configure les paramètres d'entraînement pour fine-tuner un modèle, en définissant le répertoire de sortie, le nombre d'époques (20), la désactivation de l'évaluation et des rapports externes..."
      ],
      "metadata": {
        "id": "CrZOdx0_4wqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "output_dir = '/content/drive/MyDrive/Test/fine_tuned_model'\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=20,\n",
        "    per_device_train_batch_size=16,\n",
        "    evaluation_strategy='no',\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=500,\n",
        "    report_to=\"none\"  # Désactive le rapport à W&B\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVqlNZl_RU-L",
        "outputId": "3cc3e8ca-3e77-4c5c-8b46-6516065471d2"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Le Trainer facilite l'entraînement du modèle en utilisant ces paramètres et s'assure que le modèle est correctement tokenisé et optimisé pour la tâche de NER (Reconnaissance d'Entités Nommées).\n"
      ],
      "metadata": {
        "id": "MuflVUz55fRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,                         # Modèle Camembert\n",
        "    args=training_args,                   # Arguments d'entraînement\n",
        "    train_dataset=train_dataset,         # Dataset d'entraînement\n",
        "    tokenizer=tokenizer  ,                # Tokenizer Camembert\n",
        ")"
      ],
      "metadata": {
        "id": "2H9AE4KcpJk9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f58ba7b-a49a-4a0e-e5bd-91ce3d04b6d9"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-117-12c3cc2ba02c>:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etape d'apprentissage"
      ],
      "metadata": {
        "id": "0Gqb0gq25pp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "mvk5mTKu0Z-k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "2a847557-898f-42e3-ce3c-5290b49c8904"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/20 04:38, Epoch 20/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=20, training_loss=2.3585676193237304, metrics={'train_runtime': 304.0762, 'train_samples_per_second': 0.132, 'train_steps_per_second': 0.066, 'total_flos': 10454043648000.0, 'train_loss': 2.3585676193237304, 'epoch': 20.0})"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# Sauvegarder le modèle et le tokenizer\n",
        "trainer.save_model(output_dir)  # Sauvegarde le modèle\n",
        "tokenizer.save_pretrained(output_dir)  # Sauvegarde le tokenizer"
      ],
      "metadata": {
        "id": "6ezblESG55fl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d85c98c-11f7-42c2-b5d6-fd2028bb43d4"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/Test/fine_tuned_model/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/Test/fine_tuned_model/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/Test/fine_tuned_model/sentencepiece.bpe.model',\n",
              " '/content/drive/MyDrive/Test/fine_tuned_model/added_tokens.json',\n",
              " '/content/drive/MyDrive/Test/fine_tuned_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemple de phrase à prédire (déjà tokenisée en mots)\n",
        "sentence = tokenize_sentence_french(text2)\n",
        "# Tokenizer la phrase avec gestion des sous-mots\n",
        "inputs = tokenizer(\n",
        "    sentence,\n",
        "    return_tensors=\"pt\",\n",
        "    is_split_into_words=True,\n",
        "    padding=True,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# Récupérer les word_ids pour l'alignement\n",
        "word_ids = inputs.word_ids(batch_index=0)\n",
        "\n",
        "# Faire des prédictions\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Traitement des prédictions\n",
        "predictions = torch.argmax(outputs.logits, dim=-1)[0].tolist()\n",
        "# Alignement des labels avec les mots originaux\n",
        "final_labels = []\n",
        "current_word_id = -1\n",
        "\n",
        "for idx, (word_id, pred) in enumerate(zip(word_ids, predictions)):\n",
        "    if word_id is None:  # Ignorer [CLS], [SEP], etc.\n",
        "        continue\n",
        "    if word_id != current_word_id:\n",
        "        # Nouveau mot: récupérer le label du premier sous-mot\n",
        "        final_labels.append(model.config.id2label[pred])\n",
        "        current_word_id = word_id\n",
        "\n",
        "print(probabilities)\n",
        "# Affichage des résultats alignés\n",
        "\n",
        "print(\"\\nDétail d'alignement:\")\n",
        "\n",
        "for word, label in zip(sentence, final_labels):\n",
        "    print(f\"{word} -> {label}\")"
      ],
      "metadata": {
        "id": "DzcsSwFdYW2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exportation"
      ],
      "metadata": {
        "id": "ch33aOhV6O1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Dictionnaire des désignations associées aux labels\n",
        "label_designations = {\n",
        "    \"BAIL\": \"Bailleur\",\n",
        "    \"ADDRB\": \"Adresse du Bailleur\",\n",
        "    \"NUMB\": \"Numéro du Bailleur\",\n",
        "    \"MAILB\": \"Email du Bailleur\",\n",
        "    \"PREN\": \"Preneur\",\n",
        "    \"ADDRP\": \"Adresse du Preneur\",\n",
        "    \"NUMP\": \"Numéro du Preneur\",\n",
        "    \"MAILP\": \"Email du Preneur\",\n",
        "    \"DES\": \"Description\",\n",
        "    \"SURF\": \"Surface\",\n",
        "    \"DURE\": \"Durée\",\n",
        "    \"DPE\": \"Diagnostic de Performance Énergétique\",\n",
        "    \"FDB\": \"Frais de Baux\"\n",
        "}\n",
        "\n",
        "def save_results_to_csv(tokens_list, labels_list, filename=\"results.csv\"):\n",
        "    # Vérification de la correspondance des longueurs entre tokens et labels\n",
        "    if len(tokens_list) != len(labels_list):\n",
        "        raise ValueError(\"Les longueurs des tokens et des labels doivent être identiques.\")\n",
        "\n",
        "    with open(filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"Label\", \"Token\"])  # Écriture de l'en-tête (label comme clé)\n",
        "\n",
        "        # Parcours des tokens et labels\n",
        "        for token, label in zip(tokens_list, labels_list):\n",
        "            # Vérification si le label est égal à \"0\" et si oui, on passe à l'itération suivante\n",
        "            if label != \"O\":\n",
        "                formatted_label = label_designations.get(label, label)  # Utilisation du label comme fallback si non trouvé\n",
        "                writer.writerow([formatted_label, token])  # Écriture du label formaté et du token\n",
        "\n",
        "    print(f\"Résultats sauvegardés dans {filename}\")\n"
      ],
      "metadata": {
        "id": "WoC-oOgXwiWe"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OTpCPAKQLTji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fNsfkiYN6Nv0"
      }
    }
  ]
}