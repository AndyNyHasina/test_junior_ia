# -*- coding: utf-8 -*-
"""Untitled13 (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nbc1mz_FQUu1f3TJaGV8Cf8itf2g183a
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install datasets
!apt install -y tesseract-ocr tesseract-ocr-fra
!pip install pytesseract pdf2image pillow
!apt install -y poppler-utils
!pip install NTLK
!pip install pymupdf

import pytesseract
from pdf2image import convert_from_path

"""### Extraction des donnee

Ce code permet d'extraire du texte d'un document PDF en utilisant la reconnaissance optique de caractères (OCR) grâce à Tesseract. Il commence par convertir chaque page du PDF en image, puis applique l'OCR sur ces images pour récupérer le texte. Ce texte est ensuite stocké dans un fichier afin d’être facilement exploitable. Ensuite, la fonction lire_fichier est utilisée pour lire le contenu d'un fichier texte. Elle prend en entrée le chemin du fichier, tente de l'ouvrir en mode lecture avec un encodage UTF-8, et retourne son contenu. Si le fichier est introuvable, un message d'erreur est affiché, et la fonction retourne None.  Si le fichier est correctement lu, son contenu est retourné pour être utilisé ou affiché.


"""

drive_path="/content/drive/MyDrive"

pdf_path1 = f"{drive_path}/Test/Bail 1.pdf"

pdf_path2 = f"{drive_path}/Test/Bail 2.pdf"

import fitz  # PyMuPDF

def extract_text_from_pdf(pdf_path):
    # Ouvre le fichier PDF
    document = fitz.open(pdf_path)

    text = ""

    # Parcours de toutes les pages du PDF
    for page_num in range(len(document)):
        page = document.load_page(page_num)  # Charge la page
        text += page.get_text()  # Ajoute le texte de la page à la variable 'text'

    return text

from PIL import Image
import pytesseract

def extract_text_from_image(pdf_path, output_file):
# Convertir le PDF en images
  images = convert_from_path(pdf_path)

# Extraire le texte avec Tesseract OCR
  text = ""
  for img in images:
    text += pytesseract.image_to_string(img, lang="fra") + "\n\n"

# Sauvegarde le texte extrait
  with open(output_file, "w", encoding="utf-8") as f:
    f.write(text)

  print(f"Le texte a été extrait et sauvegardé dans {output_file}")
  return output_file

print(extract_text_from_image(pdf_path1,f"{drive_path}/Test/Bail 1.txt"))
extract_text_from_image(pdf_path2,f"{drive_path}/Test/Bail 2.txt")

def lire_fichier(chemin_fichier):
    try:
        with open(chemin_fichier, "r", encoding="utf-8") as fichier:
            contenu = fichier.read()
        return contenu
    except FileNotFoundError:
        print(f"Erreur : Le fichier '{chemin_fichier}' n'existe pas.")
        return None
    except Exception as e:
        print(f"Erreur lors de la lecture du fichier : {e}")
        return None

text1 = lire_fichier(f"{drive_path}/Test/Bail 1.txt")
text2 = lire_fichier(f"{drive_path}/Test/Bail 2.txt")

import re

def extract_words_with_indices(text):
    words = []
    for match in re.finditer(r'\S+', text):  # \S+ capture les mots sans espaces
        word = match.group()
        start_idx = match.start()
        end_idx = match.end()
        words.append((word, start_idx, end_idx))
    return words

# Exemple d'utilisation

result = extract_words_with_indices(text1)
print(result)
result2 = extract_words_with_indices(text2)
#print(result2)

"""# fine-Tuning"""

import nltk
nltk.download('punkt')
nltk.download('punkt_tab')
def tokenize_sentence_french(sentence):
    # Tokenisation avec NLTK pour le français
    tokens = nltk.word_tokenize(sentence, language='french')
    return tokens

# Exemple d'utilisation

"""# Creation du Dataset
Ce code prépare des données pour entraîner un modèle de reconnaissance d'entités nommées (NER) en utilisant Camembert pour effectuer un fine-tuning sur des textes annotés. Il commence par définir des annotations sous forme de textes et leurs entités correspondantes. Ensuite, il convertit ces annotations en format IOB (Inside, Outside, Beginning), qui est un format standard utilisé pour les tâches de NER. Le code crée ensuite un dataset Hugging Face en utilisant ces annotations et étiquettes.

Le modèle Camembert (de type CamembertForTokenClassification) est ensuite chargé, ainsi que son tokenizer. Les tokens sont traités avec le tokenizer Camembert et les étiquettes sont alignées avec les tokens correspondants.

Enfin, le dataset est préparé pour l’entraînement en appliquant la fonction de tokenisation et d'alignement sur toutes les entrées du dataset.
"""

import re
import torch
from datasets import Dataset
from transformers import CamembertForTokenClassification, CamembertTokenizer, TrainingArguments, Trainer

# Exemple d'annotations
annotations = [

    {"text":text1 , "entities":[(71,86, "BAIL") , (100, 130, "ADDRB") , (144, 158, "NUMB") , (168, 193, "MAILB") , (300, 315, "PREN"),(353, 398, "ADDRP"),(412, 426, "NUMP"),(436, 462, "MAILP"),(607, 614, "DES"),(618, 621, "SURF"),(774, 782, "DURE"),(810, 825, "DPE"),(850, 866, "FDB")]},
    {"text":text2 , "entities":[(82, 95, "BAIL") , (109, 145, "ADDRB") , (159, 173, "NUMB"),(183, 206, "MAILB") , (310, 325, "PREN"),(361, 389, "ADDRP"),(403, 417, "NUMP"),(427, 448, "MAILP"),(674, 677, "SURF"),(804, 814, "DES"),(958, 966, "DURE"),(981, 995, "DPE"),(1014, 1026, "FDB")]},

]

import re

def convert_to_iob(annotation):
    text_cleaned = annotation["text"]
    tokens = tokenize_sentence_french(text_cleaned)
    # Initialisation des labels IOB
    labels = ["O"] * len(tokens)

    for start, end, label in annotation["entities"]:
        entity_text = annotation["text"][start:end]

        entity_tokens = tokenize_sentence_french(entity_text)

        # Trouver le premier index correspondant à l'entité
        try:
            nb = len(tokenize_sentence_french(annotation["text"][:start]))
            start_token_idx = tokenize_sentence_french(annotation["text"][start:]).index(entity_tokens[0])+nb
        except ValueError:
            print(f"⚠️ Entité '{entity_text}' non trouvée dans les tokens : {tokens[:50]}")
            continue
        for i in range(len(entity_tokens)):
            if start_token_idx + i < len(labels):  # Vérification pour éviter les dépassements
                labels[start_token_idx + i] = f"B-{label}" if i == 0 else f"I-{label}"

    return {"tokens": tokens, "labels": labels}


# Création du dataset
train_data = [convert_to_iob(ann) for ann in annotations]
print(train_data)

# Définition des labels
unique_labels = set(label for entry in train_data for label in entry["labels"] if label != "O")
label2id = {label: i for i, label in enumerate(["O"] + sorted(unique_labels))}
id2label = {i: label for label, i in label2id.items()}

# Transformer en dataset Hugging Face
train_dataset = Dataset.from_dict({
    "tokens": [entry["tokens"] for entry in train_data],
    "labels": [[label2id[label] for label in entry["labels"]] for entry in train_data]
})
print(  [[label2id[label] for label in entry["labels"]] for entry in train_data])

# Charger le tokenizer et le modèle Camembert
from transformers import CamembertTokenizerFast

# Charger le tokenizer rapide
tokenizer = CamembertTokenizerFast.from_pretrained("camembert-base")
model = CamembertForTokenClassification.from_pretrained("camembert-base", num_labels=len(label2id), id2label=id2label, label2id=label2id)

def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(examples["tokens"], truncation=True, padding=True, is_split_into_words=True)
    labels = examples["labels"]
    aligned_labels = []
    for i, label in enumerate(labels):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        print(word_ids)

        aligned_label = []
        prev_word_idx = None
        for word_idx in word_ids:
            if word_idx is None:
                aligned_label.append(-100)  # Ignore token
            elif word_idx != prev_word_idx:
                aligned_label.append(label[word_idx])  # Premier token du mot
            else:
                aligned_label.append(label[word_idx])  # Autres tokens du mot
            prev_word_idx = word_idx
        aligned_labels.append(aligned_label)

    tokenized_inputs["labels"] = aligned_labels
    return tokenized_inputs

# Appliquer la tokenisation
train_dataset = train_dataset.map(tokenize_and_align_labels, batched=True)


# Afficher les premières données tokenisées

"""Ce code configure les paramètres d'entraînement pour fine-tuner un modèle, en définissant le répertoire de sortie, le nombre d'époques (20), la désactivation de l'évaluation et des rapports externes..."""

from transformers import TrainingArguments
output_dir = '/content/drive/MyDrive/Test/fine_tuned_model'

training_args = TrainingArguments(
    output_dir=output_dir,
    num_train_epochs=20,
    per_device_train_batch_size=16,
    evaluation_strategy='no',
    logging_dir='./logs',
    logging_steps=500,
    report_to="none"  # Désactive le rapport à W&B
)

""" Le Trainer facilite l'entraînement du modèle en utilisant ces paramètres et s'assure que le modèle est correctement tokenisé et optimisé pour la tâche de NER (Reconnaissance d'Entités Nommées).

"""

trainer = Trainer(
    model=model,                         # Modèle Camembert
    args=training_args,                   # Arguments d'entraînement
    train_dataset=train_dataset,         # Dataset d'entraînement
    tokenizer=tokenizer  ,                # Tokenizer Camembert
)

"""# Etape d'apprentissage"""

trainer.train()

import os
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

# Sauvegarder le modèle et le tokenizer
trainer.save_model(output_dir)  # Sauvegarde le modèle
tokenizer.save_pretrained(output_dir)  # Sauvegarde le tokenizer

# Exemple de phrase à prédire (déjà tokenisée en mots)
sentence = tokenize_sentence_french(text2)
# Tokenizer la phrase avec gestion des sous-mots
inputs = tokenizer(
    sentence,
    return_tensors="pt",
    is_split_into_words=True,
    padding=True,
    truncation=True
)

# Récupérer les word_ids pour l'alignement
word_ids = inputs.word_ids(batch_index=0)

# Faire des prédictions
with torch.no_grad():
    outputs = model(**inputs)

# Traitement des prédictions
predictions = torch.argmax(outputs.logits, dim=-1)[0].tolist()
# Alignement des labels avec les mots originaux
final_labels = []
current_word_id = -1

for idx, (word_id, pred) in enumerate(zip(word_ids, predictions)):
    if word_id is None:  # Ignorer [CLS], [SEP], etc.
        continue
    if word_id != current_word_id:
        # Nouveau mot: récupérer le label du premier sous-mot
        final_labels.append(model.config.id2label[pred])
        current_word_id = word_id

print(probabilities)
# Affichage des résultats alignés

print("\nDétail d'alignement:")

for word, label in zip(sentence, final_labels):
    print(f"{word} -> {label}")

"""# Exportation"""

import csv

# Dictionnaire des désignations associées aux labels
label_designations = {
    "BAIL": "Bailleur",
    "ADDRB": "Adresse du Bailleur",
    "NUMB": "Numéro du Bailleur",
    "MAILB": "Email du Bailleur",
    "PREN": "Preneur",
    "ADDRP": "Adresse du Preneur",
    "NUMP": "Numéro du Preneur",
    "MAILP": "Email du Preneur",
    "DES": "Description",
    "SURF": "Surface",
    "DURE": "Durée",
    "DPE": "Diagnostic de Performance Énergétique",
    "FDB": "Frais de Baux"
}

def save_results_to_csv(tokens_list, labels_list, filename="results.csv"):
    # Vérification de la correspondance des longueurs entre tokens et labels
    if len(tokens_list) != len(labels_list):
        raise ValueError("Les longueurs des tokens et des labels doivent être identiques.")

    with open(filename, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["Label", "Token"])  # Écriture de l'en-tête (label comme clé)

        # Parcours des tokens et labels
        for token, label in zip(tokens_list, labels_list):
            # Vérification si le label est égal à "0" et si oui, on passe à l'itération suivante
            if label != "O":
                formatted_label = label_designations.get(label, label)  # Utilisation du label comme fallback si non trouvé
                writer.writerow([formatted_label, token])  # Écriture du label formaté et du token

    print(f"Résultats sauvegardés dans {filename}")

